#!/usr/bin/env python3
#
# Runs a docker build and extract the built container as a single flattened
# tar file. All timestamps and other non-reproducible effects of docker build
# (e.g. order of files) are squashed in order to build a reproducible tarball.
#
# The script is expected to be called with the arguments to be given to "docker build".
# The resulting tar file will be written to stdout.
#
# Call example:
#   docker_tar dockerdir --build-arg foo=bar > tree.tar
#
import argparse
import io
import json
import os
import re
import subprocess
import sys
import tarfile

image_hash_re = re.compile("((Successfully built )|(.*writing image sha256:))([0-9a-f]+).*\n")


def docker_build(args):
    """
    Runs 'docker build' and return image hash.

    Runs "docker build" with the given additional arguments to the call.
    The build logs will be passed through to stderr. Upon successful
    completion, the hash of the built docker image is returned by this
    function.
    """
    docker_args = ["docker", "build"] + args
    image_hash = None

    with subprocess.Popen(docker_args, stdout=subprocess.PIPE) as proc:
        # Pass the output generated by docker through so user potential
        # errors become visible in build logs.
        while True:
            line = proc.stdout.readline()
            if not line:
                break
            sys.stderr.buffer.write(line)
            sys.stderr.flush()

            # Try to parse the "completion" from the line received so
            # we can obtain the hash of the image.
            m = image_hash_re.match(line.decode("utf-8"))
            if m:
                image_hash = m.groups()[3]

        proc.wait()
        if proc.returncode != 0:
            raise RuntimeError("Docker build failed")

    if not image_hash:
        raise RuntimeError("Failed to obtain hash of built docker image")

    return image_hash


def _read_tar_contents(buf):
    """Reads tar file as map from filename -> content."""
    with tarfile.open(fileobj=buf, mode="r|*") as tf:

        filemap = {}
        for member in tf:
            buf = member.tobuf()  # noqa - no idea why buf is here
            if member.type == tarfile.REGTYPE:
                filemap[member.name] = tf.extractfile(member).read()
            elif (member.type == tarfile.LNKTYPE) or (member.type == tarfile.SYMTYPE):
                filemap[member.name] = member.linkname[3:]
    return filemap


def _get_layer_data(filemap):
    """Gets the docker layer data from the filemap in correct order."""
    manifest = json.loads(filemap["manifest.json"])
    layers = manifest[0]["Layers"]

    out = []
    for layer in layers:
        if isinstance(filemap[layer], str):
            out.append(filemap[filemap[layer]])
        else:
            out.append(filemap[layer])

    return tuple(out)


class Inode(object):
    def __init__(self, mode, uid, gid, uid_name, gid_name):
        self.mode = mode
        self.uid = uid
        self.gid = gid
        self.uid_name = uid_name
        self.gid_name = gid_name


class DirInode(Inode):
    def __init__(self, mode, uid, gid, uid_name, gid_name, entries={}):
        Inode.__init__(self, mode, uid, gid, uid_name, gid_name)
        self.entries = entries.copy()


class LinkInode(Inode):
    def __init__(self, mode, uid, gid, uid_name, gid_name, target):
        Inode.__init__(self, mode, uid, gid, uid_name, gid_name)
        self.target = target


class RegInode(Inode):
    def __init__(self, mode, uid, gid, uid_name, gid_name, contents):
        Inode.__init__(self, mode, uid, gid, uid_name, gid_name)
        self.contents = contents


class FS:
    def __init__(self):
        self.root = DirInode(0o755, 0, 0, "root", "root")

    def clear_dir(self, path):
        self._lookup(path).entries.clear()

    def unlink(self, path):
        basename = os.path.basename(path)
        dirname = os.path.dirname(path)

        parent = self._lookup(dirname)
        del parent.entries[basename]

    def add_dir(self, path, mode, uid, gid, uid_name, gid_name):
        basename = os.path.basename(path)
        dirname = os.path.dirname(path)

        parent = self._lookup(dirname)
        if basename in parent.entries:
            if type(parent.entries[basename]) is not DirInode:
                raise RuntimeError("Expected entry is not a directory in base layer: " + path)
        else:
            parent.entries[basename] = DirInode(mode, uid, gid, uid_name, gid_name)

    def add_link(self, path, mode, uid, gid, uid_name, gid_name, target):
        basename = os.path.basename(path)
        dirname = os.path.dirname(path)

        self._lookup(dirname).entries[basename] = LinkInode(mode, uid, gid, uid_name, gid_name, target)

    def add_reg(self, path, mode, uid, gid, uid_name, gid_name, content):
        basename = os.path.basename(path)
        dirname = os.path.dirname(path)

        self._lookup(dirname).entries[basename] = RegInode(mode, uid, gid, uid_name, gid_name, content)

    def _lookup(self, path):
        current = self.root
        for part in path.split("/"):
            if part != "":
                current = current.entries[part]
        return current

    def ls_dir(self, dir, indent=""):
        for key in sorted(dir.entries.keys()):
            print(indent + key + " " + str(dir.entries[key]))
            if type(dir.entries[key]) is DirInode:
                self.ls_dir(dir.entries[key], indent + " ")

    def ls(self):
        self.ls_dir(self.root)


def _process_layer(layer, fs):
    tf = tarfile.open(fileobj=io.BytesIO(layer), mode="r")

    # Process all members in the tarfile. They are either ordinary
    # dirs/files/symlinks to be extracted, or they are "white-out" files:
    # These direct to delete certain underlying files from previous layer.
    for member in tf:
        basename = os.path.basename(member.path)
        dirname = os.path.dirname(member.path)
        if basename == ".wh..wh..opq":
            fs.clear_dir(dirname)
        elif basename.startswith(".wh."):
            fs.unlink(os.path.join(dirname, basename[4:]))
        else:
            if member.type == tarfile.DIRTYPE:
                fs.add_dir(member.path, member.mode, member.uid, member.gid, member.uname, member.gname)
            elif member.type == tarfile.REGTYPE or member.type == tarfile.AREGTYPE:
                fs.add_reg(
                    member.path,
                    member.mode,
                    member.uid,
                    member.gid,
                    member.uname,
                    member.gname,
                    tf.extractfile(member).read(),
                )
            elif member.type == tarfile.LNKTYPE or member.type == tarfile.SYMTYPE:
                fs.add_link(
                    member.path, member.mode, member.uid, member.gid, member.uname, member.gname, member.linkname
                )
            else:
                raise RuntimeError("Unhandled tar member kind")


def docker_extract_fs(image_hash):
    """
    Extracts the image via 'docker save' and builds fs.

    Extract the docker image identified by the given hash.
    Flatten all the layers and build a temporary in-memory
    filesystem representation of the image.
    """
    with subprocess.Popen(["docker", "save", image_hash], stdout=subprocess.PIPE) as proc:
        layer_filemap = _read_tar_contents(proc.stdout)
        proc.wait()
        if proc.returncode != 0:
            raise RuntimeError("Docker save failed")

    layers = _get_layer_data(layer_filemap)
    fs = FS()
    for layer in layers:
        _process_layer(layer, fs)

    return fs


def _recurse_add_to_tar(path_prefix, dir_node, tf):
    for name in sorted(dir_node.entries.keys()):
        inode = dir_node.entries[name]
        ti = tarfile.TarInfo(path_prefix + name)
        ti.size = 0
        ti.mtime = 0
        ti.mode = inode.mode
        ti.uid = inode.uid
        ti.gid = inode.gid
        ti.uname = inode.uid_name
        ti.gname = inode.gid_name
        if type(inode) is DirInode:
            ti.type = tarfile.DIRTYPE
            tf.addfile(ti)
            _recurse_add_to_tar(path_prefix + name + "/", inode, tf)
        elif type(inode) is LinkInode:
            ti.type = tarfile.SYMTYPE
            ti.linkname = inode.target
            tf.addfile(ti)
        elif type(inode) is RegInode:
            ti.type = tarfile.AREGTYPE
            ti.size = len(inode.contents)
            tf.addfile(ti, io.BytesIO(inode.contents))
        else:
            raise RuntimeError("Unhandled inode kind")


def tar_fs(fs, outfile):
    """
    Tar up the filesystem tree.

    Recursively archive the given filesystem tree as a tar
    archive into the given file. All files are written with
    "zero" timestamps and in deterministic order in order
    to generate reproducible results.
    """
    tf = tarfile.open(fileobj=outfile, mode="w")

    _recurse_add_to_tar("", fs.root, tf)


def make_argparser():
    parser = argparse.ArgumentParser()
    parser.add_argument("-o", "--output", help="Target (tar) file to write to", type=str)
    parser.add_argument(
        "build_args",
        metavar="build_args",
        type=str,
        nargs="*",
        help="Extra args to pass to docker build",
    )
    return parser


def main():
    args = make_argparser().parse_args(sys.argv[1:])

    out_file = args.output
    build_args = list(args.build_args)

    # Build the docker image.
    build_args.append("--pull")
    if os.environ.get("CI_JOB_NAME", "").startswith("docker-build-ic"):
        build_args.append("--no-cache")
    image_hash = docker_build(build_args)

    # Extract and flatten all layers, build an in-memory pseudo filesystem
    # representing the docker image.
    fs = docker_extract_fs(image_hash)

    # Export the filesystem tree as a tar file.
    tar_fs(fs, open(out_file, "wb"))


if __name__ == "__main__":
    main()
